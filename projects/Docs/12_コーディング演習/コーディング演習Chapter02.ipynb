{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZasHaANFwpj8"},"source":["# 2. MLP基礎\n","## 概要\n","本演習ではChapter02で学習した、深層学習の基礎である多層パーセプトロンを穴埋め形式で実装します。<br>\n","予め用意されたコード（訓練用・テスト用データの取得、ミニバッチ学習など）はそのまま使用し、指示された穴埋め部（順伝播と誤差逆伝播）を編集してください。<br>\n","演習問題文は<font color=\"Red\">赤字</font>で表示されています。<br>このファイルは必ず最後までコードをすべて実行し、「最後までコードが実行可能」・「学習結果の出力がある」・「学習が成功している」の３つを満たした状態で提出してください。\n","\n","所要時間：3~5時間"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"i2fJuwinlOPU"},"source":["### 【Google colabのみ実行】ライブラリのインストール\n","\n","必要なライブラリのインストールと、実行環境のバージョンを統一します。<br>\n","\n","使用するライブラリ名とバージョンは配布資料の<font color=Red>「requirements.txt」</font>で確認できます。\n","\n","※以下のセルを実行しましたら、「ランタイム」→「ランタイムを再起動」により<font color=Red>再起動</font>を行ってください。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eW7MZHB0nm6Z"},"outputs":[],"source":["import sys\n","# Google colab環境であるか判定\n","if 'google.colab' in sys.modules:\n","    # ライブラリのインストール\n","    !pip install opencv-python==4.5.5.62\n","else:\n","    print(\"Not Google Colab\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"y2V7ToPKgqnn"},"source":["### 【Google colabのみ実行】ドライブのマウント\n","\n","Google Colabでは、**マウント**という作業を行って、Googleドライブ内のファイルを操作できる状態にする必要があります。<br>\n","<br>\n","以下のコードを実行してマウントしてください。なお、認証で**Googleアカウントへのログインが必要**になります。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VqfbG0AQhK8y"},"outputs":[],"source":["import sys\n","# Google colab環境であるか判定\n","if 'google.colab' in sys.modules:\n","    # マウントを行う\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","else:\n","    print(\"Not Google Colab\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FORhYgdjha1N"},"source":["### ファイル操作\n","\n","手持ちのデータをcolabで使用するには、データをGoogleドライブ内のフォルダに上げる必要があります。<br>\n","\n","**画面左側のファイルマークのボタン**をクリックして、ドライブ内のファイルリストを開いてください。<br>\n","\n","すると「drive」「sample_data」というフォルダがあり、「drive」がマイドライブや共有ドライブの入っているフォルダになります。<br>\n","<br>\n","この「drive」フォルダの中でアップロードしたい場所を選び、**右クリック(または3点リーダー「⋮」をクリック)から「アップロード」を押して**、今回使用するファイル<font color=\"Red\">「train_data.csv」「test_data.csv」</font>をアップしましょう。<br>\n","<br>\n","アップロードしたファイルをコードの関数内で参照する際は右クリックで「パスをコピー」を選択し、コードに貼り付けましょう。<br>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"poyscwJcwpj8"},"source":["## 回帰モデル演習（スクラッチ）"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"toSmtFwyadv8"},"source":["### データの準備"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GlCV4u18WzKQ"},"source":["#### ライブラリのインポート\n","\n","必要なライブラリをインポートします。エラーになる場合は該当するものをインストールしてください。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4gMxEGbWydq"},"outputs":[],"source":["import csv\n","import os\n","import sys\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import cv2\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","%matplotlib inline\n","# \"%matplotlib inline\" の代わりに以下のコマンドを使用できる場合、3次元で座標軸を操作可能なプロットを表示することができます。\n","# %matplotlib notebook\n","\n","# 乱数シードを指定\n","np.random.seed(seed=0)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3TQ5X8MGXAXQ"},"source":["#### データの３次元散布図を作成する関数\n","\n","データ可視化に使用します。演習の問題とは関係ありませんので読み飛ばしてください。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1w57CxJgW_3C"},"outputs":[],"source":["def plot_data(data, name='Data Plot'):\n","    # グラフ作成\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # 軸ラベルの設定\n","    ax.set_xlabel(\"X0-axis\")\n","    ax.set_ylabel(\"X1-axis\")\n","    ax.set_zlabel(\"Y-axis\")\n","\n","    # 表示範囲の設定\n","    ax.set_xlim(-2, 2)\n","    ax.set_ylim(-2, 2)\n","    ax.set_zlim(0, 10)\n","\n","    ax.plot(data[:, 0], data[:, 1], data[:, 2],\n","            \"o\", color=\"#ff2222\", ms=2, mew=0.5)\n","    ax.set_title(name)\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wNH-bUqaX5Ry"},"source":["#### データの読み込み\n","\n","本演習では、train_data.csvを訓練用データ、test_data.csvをテスト用データとして使用します。データはx0, x1, yの3列からなり、以下の式に従っています。\n","\n","$$ y = \\left\\{ \\begin{array}{ll} (x_0-1)^2 + (x_1-1)^2 & (x_0 \\gt 0) \\\\ (x_0+1)^2 + (x_1+1)^2 & (x_0 \\leq 0) \\end{array} \\right. $$\n","\n","訓練データ10000個の(x0, x1, y)の組を多層パーセプトロンで学習し、テストデータ2000個の学習におけるコストの推移を観測します。\n","まずはデータのプロットを表示してみます。\n","\n","※ここでエラーとなっている場合はライブラリのインポートが完了していないか、正常にデータを読み込めていません。同ディレクトリ内にダウンロードした`train_data.csv`と`test_data.csv`が存在していることを確認してください。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_980SHGpX4ux"},"outputs":[],"source":["# Pathの設定\n","if 'google.colab' in sys.modules:\n","    # マイドライブ内のデータを読み込むpathに設定\n","    train_path = '/content/drive/MyDrive/train_data.csv'\n","    test_path = '/content/drive/MyDrive/test_data.csv'\n","else:\n","    train_path = 'train_data.csv'\n","    test_path = 'test_data.csv'\n","\n","# データの読み込み\n","with open(train_path) as f:\n","    reader = csv.reader(f)\n","    train_data = np.array([[float(x) for x in row] for row in reader])\n","with open(test_path) as f:\n","    reader = csv.reader(f)\n","    test_data = np.array([[float(x) for x in row] for row in reader])\n","\n","# 訓練データを可視化\n","plot_data(train_data, 'Train Data')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nCT224W1YFil"},"source":["#### データの数\n","\n","訓練時とテスト時で扱うデータ数を確認します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1cjxlH_SYFAJ"},"outputs":[],"source":["N_train = train_data.shape[0]\n","print('訓練データの数: ', N_train)\n","N_test = test_data.shape[0]\n","print('テストデータの数: ', N_test)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"o_Kbt24HaoWl"},"source":["### 活性化関数"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cxi6a480HLWa"},"source":["#### ReLU関数\n","\n","* <font color=\"Red\">問1. ReLUを完成させてください。</font>\n","\n","```\n","引数:\n","    v: [N, M] (np.float)\n","       Nはバッチサイズにあたる\n","返値:\n","    [N, M] (np.float)\n","```\n","\n","- 以下の式で定義されるrelu関数を実装します。\n","$$ \\text{ReLU}(x) = \\left\\{ \\begin{array}{ll} x & (x \\gt 0) \\\\ 0 & (x \\leq 0) \\end{array} \\right. $$\n","- 引数`v`の各要素と0の大きい方を取り、`x`とします。\n","- ヒント: np.maximum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YdQtn9FRIiSW"},"outputs":[],"source":["def relu(v):\n","    x =  ### 問1 ### 旧シラバスコース演習1問1と同じ\n","    return x"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lScMpGjtHl55"},"source":["#### softmax関数\n","* <font color=\"Red\">問2. softmax関数を完成させてください。</font>\n","\n","```\n","引数:\n","    x: [N, M] (np.float)\n","       Nはバッチサイズにあたる\n","返値:\n","    [N, M] (np.float)\n","```\n","\n","バッチ計算が可能なsoftmax関数を実装します。<br>\n","exp関数がオーバーフローすることを防ぐために、各データについて入力信号の最大値を引いて、0以下にします。<br>\n","    numpyでは```[N, M] - [N]```の計算はブロードキャストができませんので、```x```を転置し```[M, N] - [N]```の形で計算を行ったあと、さらに転置をして元の形に戻します。\n","$$ \\text{Softmax}(x_{ij}) = \\frac{\\exp (x_{ij}^{\\prime})}{\\sum_{k}^{M}\\exp (x_{ik}^{\\prime})} \\\\ $$\n","$$    x_{ij}^{\\prime} = x_{ij} - max_{j}(x_{ij}) $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kY7r_2G9HfB3"},"outputs":[],"source":["def softmax(x):\n","    x = x.T\n","    x =  ### 問2-1 ###\n","    x =  ### 問2-2 ###\n","    return x.T"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"E6Km45bzLOE_"},"source":["#### sigmoid関数\n","* <font color=\"Red\">問3. sigmoid関数を完成させてください。</font>\n","\n","```\n","引数:\n","    x: [N, M] (np.float)\n","       Nはバッチサイズにあたる\n","返値:\n","    [N, M] (np.float)\n","```\n","\n","バッチ計算が可能なsigmoid関数を実装します。<br>\n","\n","$$   \\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)} $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MFTS8oOJ-cP"},"outputs":[],"source":["def sigmoid(x):\n","    return  ### 問3 ###"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"I0o64EHJQdgQ"},"source":["#### tanh関数\n","* <font color=\"Red\">問4. tanh関数を完成させてください。</font>\n","  * <font color=\"Red\">問4-1. tanh関数の分子にあたるsinh(双曲線正弦関数)の式を記述しましょう。</font>\n","  * <font color=\"Red\">問4-2. tanh関数の分母にあたるcosh(双曲線余弦関数)の式を記述しましょう。</font>\n","\n","```\n","引数:\n","    x: [N, M] (np.float)\n","       Nはバッチサイズにあたる\n","返値:\n","    [N, M] (np.float)\n","```\n","\n","バッチ計算が可能なtanh関数を実装します。<br>\n","tanh関数は以下のように定義されている。<br>\n","\n","$$   \\text{tanh}(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)} $$\n","\n","tanh関数はハイパボリックタンジェントやタンエイチと読み、sinh関数とcosh関数を用いて以下のように書き直すことができる。\n","\n","$$   \\text{tanh}(x) = \\frac{\\text{sinh}(x) }{\\text{cosh}(x) } $$\n","\n","sinh関数はハイパボリックサインやシャイン、cosh関数はハイパボリックコサインやコッシュと読み、以下のように定義されている。\n","\n","$$   \\text{sinh}(x) = \\frac{\\exp(x) - \\exp(-x)}{2} $$\n","\n","$$   \\text{cosh}(x) = \\frac{\\exp(x) + \\exp(-x)}{2} $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9YknOB0QZ-E"},"outputs":[],"source":["def tanh(x):\n","    sinh =  ### 問4-1 ###\n","    cosh =  ### 問4-2 ###\n","    return sinh / cosh"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kF11K8XMTdTr"},"source":["#### Leaky ReLU関数\n","\n","* <font color=\"Red\">問5. Leaky ReLUを完成させてください。</font>\n","\n","```\n","引数:\n","    x: [N, M] (np.float)\n","       Nはバッチサイズにあたる\n","返値:\n","    [N, M] (np.float)\n","```\n","\n","- 以下の式で定義されるLeaky ReLU関数を実装します。\n","\n","$$ \\text{Leaky ReLU}(x,\\alpha) = \\left\\{ \\begin{array}{ll} x & (x \\gt 0) \\\\ \\alpha x & (x \\leq 0) \\end{array} \\right. $$\n","- $\\alpha$は定数。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OC4PV3zcVgxk"},"outputs":[],"source":["def lrelu(x, alpha=0.01):\n","    x =  ### 問5 ###\n","    return x"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SoJM5r7VcFav"},"source":["#### コスト関数 平均二乗誤差\n","以下の式で定義される平均二乗誤差を実装します。出題範囲ではないですが、確認しておいてください。\n","```\n","引数:\n","    t: [N, M] (np.float)\n","    y: [N, M] (np.float)\n","       Nはバッチサイズにあたる\n","返値:\n","    mse: [M] (np.float)\n","```\n","\n","\n","$$\n","\\text{MSE} = \\frac{1}{NM} \\sum_{i}^{N}\\sum_{j}^{M} (t_{ij}-y_{ij})^2\n","$$\n","- `t`と`y`の差の2乗の平均を取り、`mse`とします。\n","- 今回のように、M=1（目的変数が一つ）である場合が多い"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qND0SGg-cDs2"},"outputs":[],"source":["def MSE(t, y):\n","    mse = np.mean(np.square(t - y))\n","    return mse"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XWJaA-LsbJOZ"},"source":["#### 多層パーセプトロンの定義\n","\n","<font color=\"Red\">問6-1~問6-4. MLPクラスの順伝播forward関数を完成させてください。</font>\n","- 以下の式で定義される順伝播を実装します。(Φは活性化関数)\n","$$\n","X_{i+1} = \\phi(X_i \\cdot W_i + B_i)\n","$$    \n","- 第1層`self.layer1`を定義します。入力層`self.layer0`と重み`self.w1`との行列積を取り、バイアス`self.b1`を加算します。そして活性化関数`relu`に渡します。\n","- 第2層`self.layer2`を定義します。第1層`self.layer1`と重み`self.w2`との行列積を取り、バイアス`self.b2`を加算します。そして活性化関数`relu`に渡します。\n","- 第3層`self.layer3`を定義します。第2層`self.layer2`と重み`self.w3`との行列積を取り、バイアス`self.b3`を加算します。そして活性化関数`relu`に渡します。\n","- 出力層`self.out`を定義します。第3層`self.layer3`と重み`self.w4`との行列積を取り、バイアス`self.b4`を加算します。\n","- 出力層では活性化関数Φは使用しないことに注意せよ\n","- ヒント: np.dot()　もしくは np.matmul()\n","\n","<font color=\"Red\"> 問6-5~問6-8. MLPクラスの逆伝播によって誤差と勾配を求めるbackward関数を完成させてください。</font>\n","- 出力層誤差`delta4`を定義します。二乗誤差の微分なので、以下の式に従います。  <br>\n","$$\n","\\delta_{out} = \\frac{d(T-Y)^2}{dY} = -2(T-Y)\n","$$　\n","- 誤差逆伝播は以下の式に従います。\n","$$\n","\\delta_i = \\phi'(v_{i+1})*\\delta_{i+1} \\cdot W_{i+1}^t\n","$$\n","$$\n","\\phi'(v_i) = \\left\\{ \\begin{array}{ll}  \\frac{d}{dv_i}v_i & (v_i \\gt 0) \\\\  \\frac{d}{dv_i}0 & (v_i \\leq 0) \\end{array} \\right. = \n","                \\left\\{ \\begin{array}{ll} 1 & (v_i \\gt 0) \\\\ 0 & (v_i \\leq 0) \\end{array} \\right.\n","$$\n","- 第3層誤差`delta3`を定義します。出力層誤差`delta4`と重み`self.w4`の転置との行列積を取ります。\n","- 第2層誤差`delta2`を定義します。第3層誤差`delta3`と第3層`self.layer3`におけるreluの微分との積をとり、重み`self.w3`の転置との行列積を取ります。\n","- 第1層誤差`delta1`を定義します。第2層誤差`delta2`と第2層`self.layer2`におけるreluの微分との積をとり、重み`self.w2`の転置との行列積を取ります。\n","- ヒント: numpy.dot(), numpy.transpose(), numpy.where()\n","- ヒント: reluの微分は該当する層の各要素が0より大きいものは1, 0以下のものは0としたベクトルに等しい"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJrqfSU5a5g5"},"outputs":[],"source":["# 回帰モデル\n","class MLP_regressor():\n","    '''\n","    多層パーセプトロン Multi Layered Perceptron\n","    構成: [入力層, 第1層, 第2層, 第3層, 出力層]\n","    ノード数: [2, 50, 50, 10, 1]\n","    '''\n","\n","    def __init__(self):\n","        '''\n","        コンストラクタ\n","        パラメータ（重みw, バイアスb）の定義\n","        第1層重み self.w1: [2, 50] 平均0, 標準偏差0.1の乱数\n","        第2層重み self.w2: [50, 50] 平均0, 標準偏差0.1の乱数\n","        第3層重み self.w3: [50, 10] 平均0, 標準偏差0.1の乱数\n","        第4層重み self.w4: [10, 1] 平均0, 標準偏差0.1の乱数\n","        \n","        第1層バイアス self.b1: [50] 要素が全て0\n","        第2層バイアス self.b2: [50] 要素が全て0\n","        第3層バイアス self.b3: [10] 要素が全て0\n","        第4層バイアス self.b4: [1] 要素が全て0\n","        \n","        numpyの乱数については以下のページを参照\n","        https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html\n","        '''\n","        \n","        # 重みの定義\n","        self.w1 = np.random.randn(2, 50) * 0.1\n","        self.w2 = np.random.randn(50, 50) * 0.1\n","        self.w3 = np.random.randn(50, 10) * 0.1\n","        self.w4 = np.random.randn(10, 1) * 0.1\n","\n","        # バイアスの定義\n","        self.b1 = np.zeros(50, dtype=float)\n","        self.b2 = np.zeros(50, dtype=float)\n","        self.b3 = np.zeros(10, dtype=float)\n","        self.b4 = np.zeros(1, dtype=float)\n","\n","    def forward(self, x):\n","        '''\n","        順伝播\n","        入力 x: [N, 2]\n","        入力層 self.layer0: [N, 2]\n","        第1層 self.layer1: [N, 50]\n","        第2層 self.layer2: [N, 50]\n","        第3層 self.layer3: [N, 10]\n","        出力層 self.out: [N, 1]\n","        '''\n","        \n","        self.layer0 = x\n","        self.layer1 = ### 問6-1 ### 旧シラバスコース演習1問3と同じ\n","        self.layer2 = ### 問6-2 ### 旧シラバスコース演習1問4と同じ\n","        self.layer3 = ### 問6-3 ### 旧シラバスコース演習1問5と同じ\n","        self.out = ### 問6-4 ### 旧シラバスコース演習1問6と同じ\n","        return self.out\n","\n","    def backward(self, t, y):\n","        '''\n","        逆伝播\n","        真の値 t: [N, 1]\n","        予測値 y: [N, 1]\n","        \n","        出力層誤差 delta4: [N, 1]\n","        第3層誤差 delta3: [N, 10]\n","        第2層誤差 delta2: [N, 50]\n","        第1層誤差 delta1: [N, 50]\n","        \n","        第4層b勾配 dedb4: [N, 1]\n","        第3層b勾配 dedb3: [N, 10]\n","        第2層b勾配 dedb2: [N, 50]\n","        第1層b勾配 dedb1: [N, 50]\n","        \n","        第4層w勾配 dedw4: [N, 10, 1]\n","        第3層w勾配 dedw3: [N, 50, 10]\n","        第2層w勾配 dedw2: [N, 50, 50]\n","        第1層w勾配 dedw1: [N, 2, 50]\n","        '''\n","        \n","        # 出力層の誤差デルタは二乗誤差の微分\n","        delta4 = ### 問6-5 ### 旧シラバスコース演習1問7と同じ\n","        # 誤差逆伝播\n","        delta3 = ### 問6-6 ### 旧シラバスコース演習1問8と同じ\n","        delta2 = ### 問6-7 ### 旧シラバスコース演習1問9と同じ\n","        delta1 = ### 問6-8 ### 旧シラバスコース演習1問10と同じ\n","\n","        # バイアスbのコスト関数eに対する勾配\n","        self.dedb4 = np.mean(delta4, axis=0)\n","        self.dedb3 = np.mean(delta3 * (self.layer3 > 0), axis=0)\n","        self.dedb2 = np.mean(delta2 * (self.layer2 > 0), axis=0)\n","        self.dedb1 = np.mean(delta1 * (self.layer1 > 0), axis=0)\n","\n","        # 重みwのコスト関数eに対する勾配\n","        self.dedw4 = np.dot(self.layer3.T, delta4) / delta4.shape[0]\n","        self.dedw3 = np.dot(self.layer2.T, delta3 * (self.layer3 > 0)) / delta3.shape[0]\n","        self.dedw2 = np.dot(self.layer1.T, delta2 * (self.layer2 > 0)) / delta2.shape[0]\n","        self.dedw1 = np.dot(self.layer0.T, delta1 * (self.layer1 > 0)) / delta1.shape[0]\n","\n","    def optimize_GradientDecent(self, lr):\n","        '''\n","        勾配降下法によるパラメータの更新\n","        '''\n","        self.b1 -= lr * self.dedb1\n","        self.b2 -= lr * self.dedb2\n","        self.b3 -= lr * self.dedb3\n","        self.b4 -= lr * self.dedb4\n","\n","        self.w1 -= lr * self.dedw1\n","        self.w2 -= lr * self.dedw2\n","        self.w3 -= lr * self.dedw3\n","        self.w4 -= lr * self.dedw4"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_b61sXe9bmEa"},"source":["#### 学習\n","\n","以下ではこれまでで定義した多層パーセプトロンを使用し、データを学習します。\n","コストが正常に減少し、500epochで0.5以下にまで到達していれば学習は成功していると言えます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slaBG8Xobk_P"},"outputs":[],"source":["# モデルの定義\n","model = MLP_regressor()\n","\n","# 学習率\n","lr = 0.01\n","# 学習エポック数\n","n_epoch = 500\n","\n","x_train = train_data[:, 0:2]\n","t_train = train_data[:, 2:3]\n","x_test = test_data[:, 0:2]\n","t_test = test_data[:, 2:3]\n","\n","# n_epoch繰り返す\n","for n in range(n_epoch):\n","    # 訓練\n","    # Chapter02範囲外のため、ミニバッチは使用しない\n","    y = model.forward(x_train)\n","    train_loss = MSE(t_train, y)\n","    model.backward(t_train, y)\n","    model.optimize_GradientDecent(lr)\n","\n","    # テスト\n","    y = model.forward(x_test)\n","    test_loss = MSE(t_test, y)\n","\n","    print('EPOCH ', n + 1, ' | TRAIN LOSS ',\n","          train_loss, ' | TEST LOSS ', test_loss)\n","regression_loss = test_loss"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"v6GOoBM7bqOv"},"source":["#### 予測データの散布図\n","\n","予測値の散布図と訓練データの散布図を比較してみましょう。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eC8XIEkAbr3l"},"outputs":[],"source":["y = model.forward(x_test)\n","predict_data = np.concatenate([x_test, y], axis=1)\n","plot_data(predict_data, 'Predict Data')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ru6K-JkAwpj8"},"source":["## 分類モデル演習（スクラッチ）\n","\n","---"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RrJK3aY1-8dC"},"source":["### データ準備"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UocOHTjtfF4i"},"source":["回帰モデル演習では層ごとに変数を定義し全計算を実装しましたが、分類モデル演習ではクラスと計算グラフを利用し、より汎用的なモデルを設計します。\n","\n","#### データの読み込み\n","本演習では、MNISTを使用し、全結合ネットワークで手書き数字の画像の10分類を行います。\n","\n","今回はデータセットとして、\"MNIST\"を用います。\n","\n","\"MNIST\"は0~9の手書き数字が画像になった画像データと書かれた数字の正解ラベルで構成されており、チュートリアルでよく使われています。データセット全体は70000件で構成されています。\n","\n","`sklearn`の`train_test_split`を用いてMNISTデータを訓練データとテストデータに分割します。\n","\n","引数`test_size=0.2`と指定することで、訓練データ:テストデータ = 8:2 に分割することが出来ます。\n","\n","データは 1チャンネル 28×28 の配列となっています。\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fi9tFnnpfRIc"},"outputs":[],"source":["# mnistデータセットのロード(ネットワーク接続が必要・少し時間がかかります)\n","if os.path.exists('mnist_784'):\n","    with open('mnist_784','rb') as f:\n","        mnist = pickle.load(f)\n","else:\n","    mnist = datasets.fetch_openml('mnist_784', as_frame=False)\n","    with open('mnist_784', 'wb') as f:\n","        pickle.dump(mnist, f)\n","        \n","# 画像とラベルを取得\n","X, T = mnist.data, mnist.target\n","# 訓練データとテストデータに分割\n","X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m38qlSsYfTHJ"},"outputs":[],"source":["# ラベルデータをint型にし、one-hot-vectorに変換します\n","T_train = np.eye(10)[T_train.astype(\"int\")]\n","T_test = np.eye(10)[T_test.astype(\"int\")]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KIMb4eQzfWsM"},"source":["#### one-hot-vectorとは？\n","たとえば$a$が，0~9の整数のみを含むベクトルだとわかっている時に、各要素を数字に該当する列の要素のみが1、その他が0となるようなベクトルにする。\n","$$\n","\\begin{equation*}\n","a=\n","\\begin{pmatrix}\n","1\\\\\n","8\\\\\n","4\\\\\n","2\\\\\n","0\n","\\end{pmatrix}\\to\n","a\\_onehot = \n","\\begin{pmatrix}\n","0, 1, 0, 0, 0, 0, 0, 0, 0, 0\\\\\n","0, 0, 0, 0, 0, 0, 0, 0, 1, 0\\\\\n","0, 0, 0, 0, 1, 0, 0, 0, 0, 0\\\\\n","0, 0, 1, 0, 0, 0, 0, 0, 0, 0\\\\\n","1, 0, 0, 0, 0, 0, 0, 0, 0, 0\n","\\end{pmatrix}\n","\\end{equation*}\n","$$\n","学習する正解ラベルデータは，one-hot-vectorで表されることが多い．"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"a0IwfNCXfZIa"},"source":["#### データの構造\n","データ数、画像データXの形、ラベルTの形などを調べます。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('訓練データの画像の形:', X_train.shape)\n","print('テストデータの画像の形:', X_test.shape)\n","\n","print(\"\\n\"'訓練データのラベルTの形:', T_train.shape)\n","print('テストデータのラベルTの形:', T_test.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TPFS01Y0ffV2"},"source":["#### データのサンプリング\n","画像・ラベルデータをランダムにいくつか取り出して可視化します。\n","画像は784要素の1次元ベクトルとしてXに格納されていますが、画像として表示するときは28x28の二次元にreshapeします。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PdvA1qlUfU6u"},"outputs":[],"source":["# テストデータをランダムサンプリング\n","perm = np.random.permutation(len(X_test))\n","# サンプル画像を表示する\n","plt.gray()\n","for i in perm[:3]:\n","    plt.imshow(X_test[perm[i]].reshape(28, 28))\n","    plt.show()\n","    print('Label: ', np.argmax(T_test[perm[i]]))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jd1YSnMf_OKQ"},"source":["### 多層パーセプトロンの定義"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-Vk5jpEjfmuy"},"source":["#### クロスエントロピー誤差\n","以下の式で定義される平均二乗誤差を実装します。出題範囲ではないですが、確認しておいてください。<br>\n","```\n","引数:\n","    t: [N, M] (np.float)\n","    y: [N, M] (np.float)\n","返値:\n","    error: (np.float)\n","```\n","\n","```y == 0```のときlog関数が破綻しないよう、$ y $ に小さな値 $ \\delta = 10^{-8}$ を加算します。\n","error は 以下の式に従います。\n","$$ error = -\\frac {1}{N} \\sum_{i}^{N} {t_i * \\log{(y_i + \\delta)}} $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7ufABUNfiiV"},"outputs":[],"source":["def cross_entropy_error(t, y):\n","    delta = 1e-8\n","    batch_size = t.shape[0]\n","    error = -np.sum(t * np.log(y + delta)) / batch_size\n","    return error"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xI5eIqICfvqW"},"source":["#### ソフトマックスクロスエントロピー誤差\n","ソフトマックスクロスエントロピー誤差のクラスを実装します。出題範囲ではないですが、確認しておいてください。<br>\n","順伝播```__call__(self, t, y)```ではyのソフトマックスを取り、tとのクロスエントロピー誤差を返します。その際にyとtをインスタンス変数self.yとself.tに記憶します。関数名を```__call__```としているのは、**関数オブジェクト**を作ることで\"インスタンス名()\"で順伝播を呼び出せるようにするためです。<br>\n","逆伝播```backward(self)```では、順伝播で記憶されたself.yとself.tを使用して誤差に対するyの勾配dyを計算します。<br>\n","yの勾配は以下の式に従います。<br>\n","※コードの```dy```と式の$ dy $は意味が異なり、```dy``` = $ \\frac {dL(t, y)}{dy} $であることに注意してください。\n","$$ \\frac {dL(t, y)}{dy} = y - t $$\n","また、以降の計算ではバッチの平均を取るため、dyはバッチサイズで割ってから返します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iw9sqMo8fqiu"},"outputs":[],"source":["class SoftmaxCrossEntropyLoss():\n","    def __init__(self):\n","        self.y = None\n","        self.t = None\n","        self.loss = None\n","        \n","    def __call__(self, t, y):\n","        self.y = softmax(y)\n","        self.t = t.copy()\n","        self.loss = cross_entropy_error(self.t, self.y)\n","        return self.loss\n","    \n","    def backward(self):\n","        batch_size = self.t.shape[0]\n","        dy = self.y - self.t\n","        dy /= batch_size\n","        return dy"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZEoZImPtf-Zd"},"source":["#### 全結合層\n","\n","```\n","インスタンス変数:\n","    self.w: [M, L] (np.float)\n","            層の重み。正規分布で初期化。\n","    self.b: [L] (np.float)\n","            層のバイアス。ゼロで初期化。\n","    self.x: [N, M] (np.float)\n","            層の入力信号を記録。Nはバッチサイズ。\n","    self.dw: [M, L] (np.float)\n","            層の重みの勾配\n","    self.db: [L] (np.float)\n","            層のバイアスの勾配\n","            \n","__call__(self, x):\n","    引数:\n","        x: [N, M] (np.float)\n","            入力信号\n","    返値:\n","        out: [N, L] (np.float)\n","            出力信号\n","\n","backward(self, dout):\n","    引数:\n","        dout: [N, L] (np.float)\n","            出力信号の勾配\n","    返値:\n","        dx: [N, M] (np.float)\n","            入力信号の勾配\n","        \n","```\n","順伝播```__call__(self, x)```は入力信号xを層の重み```self.w```とバイアス```self.b```でアフィン変換し出力とします。\n","計算は以下の式に従います。\n","$$ \\text{Affine}(x) = out = x \\cdot w + b $$\n","逆伝播```backward(self, dout)```は出力側の勾配[L, M]を入力側に逆伝播させます。入力の勾配```dx```、重みの勾配```dw```、バイアスの勾配```db```それぞれを計算し、```dx```を返します。\n","計算は以下の式に従います。\n","\n","$$ \n","\\begin{align}\n","\\text{grad}(x_{ij}) &= \\sum_{k} \\text{grad}(out_{ik}) \\frac{dout_{ik}}{dx_{ij}} \\\\\n","\\text{grad}(x_{ij}) &= \\sum_{k}\\text{grad}(out_{ik})w_{jk} \\\\\n","\\text{grad}(x) &= \\text{grad}(out) \\cdot w^T \\\\ \\space \\\\\n","\\end{align}\n","$$ \n","\n","$$ \n","\\begin{align}\n","\\text{grad}(w_{jk}) &= \\sum_{i} \\text{grad}(out_{ik}) \\frac{dout_{ik}}{dw_{jk}} \\\\\n","\\text{grad}(w_{jk}) &= \\sum_{i} \\text{grad}(out_{ik})x_{ij} \\\\\n","\\text{grad}(w) &= x^T \\cdot \\text{grad}(out) \\\\ \\space \\\\\n","\\end{align}\n","$$\n","\n","$$ \n","\\begin{align}\n","\\text{grad}(b_{k}) &= \\sum_{i} \\text{grad}(out_{ik}) \\frac{dout_{ik}}{db_{k}} \\\\\n","\\text{grad}(b) &= \\sum_{i} \\text{grad}(out_{i}) \n","\\end{align}\n","$$\n","\n","\n","    \n","実装コードでは$ \\text{grad}(x) $ = ```dx```, $ \\text{grad}(w) $ = ```dw```, $ \\text{grad}(b) $ = ```db```と命名されています。 \n","\n","* <font color=\"Red\">問7. 全結合層クラスを完成させてください。</font>\n","  * <font color=\"Red\">問7-1. self.x, self.w, self.bの3つの変数を用いて順伝播の出力式を記述してください。</font>\n","  * <font color=\"Red\">問7-2. 出力信号の勾配doutとself.wを用いて入力信号の勾配を算出する式を記述しましょう。</font>\n","  * <font color=\"Red\">問7-3. 逆伝播に使う勾配self.dwを算出する式を記述しましょう。</font>\n","  * <font color=\"Red\">問7-4. 逆伝播に使う勾配self.dbを算出する式を記述しましょう。</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_yZDXZ0f75f"},"outputs":[],"source":["class FullyConnectedLayer():\n","    def __init__(self, input_shape, output_shape):\n","        self.w = np.random.randn(input_shape, output_shape) * 0.01\n","        self.b = np.zeros(output_shape, dtype=np.float)\n","        self.x = None\n","        self.dw = None\n","        self.db = None\n","        \n","    def __call__(self, x):\n","        self.x = x\n","        out =  ### 問7-1 ### 旧シラバスコース演習2問5と同じ\n","        return out\n","    \n","    def backward(self, dout):\n","        dx =  ### 問7-2 ### 旧シラバスコース演習2問6と同じ\n","        self.dw =  ### 問7-3 ### 旧シラバスコース演習2問7と同じ\n","        self.db =  ### 問7-4 ### 旧シラバスコース演習2問8と同じ\n","        return dx"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Dnh-VU6q-a7N"},"source":["#### ReLUクラス\n","\n","```\n","インスタンス変数:\n","    self.mask: [N, M] np.bool\n","               マスクされるxのフラッグ\n","               \n","__call__(self, x):\n","    引数:\n","        x: [N, M] np.float\n","    返値:\n","        out: [N, M] np.float\n","\n","backward(self, dout):\n","    引数:\n","        dout: [N, M] np.float\n","    返値:\n","        dx: [N, M] np.float\n","    \n","```\n","\n","ReLUの順伝播と逆伝播をクラスで実装します。\n","順伝播```__call__(self, x)```は入力信号xに対して、```x <= 0```部分が1、それ以外は0となるような行列self.maskを定義します。numpy配列の**ブールインデックス参照**を利用し、xのマスク部を0に変換し返します。\n","逆伝播```backward(self, dout)```は出力信号の勾配doutの保存されたマスク部を0に変換し、dxとして入力信号の勾配を返します。\n","\n","* <font color=\"Red\">問8. ReLUクラスを完成させてください。</font>\n","  * <font color=\"Red\">問8-1. マスク部のフラグであるself.maskを用いて、xのマスク部を0に変換する式を記述しましょう。</font>\n","  * <font color=\"Red\">問8-2. マスク部のフラグであるself.maskを用いて、出力信号の勾配doutのマスク部を0に変換する式を記述しましょう。</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jja7vZi-VlI"},"outputs":[],"source":["class ReLU():\n","    def __init__(self):\n","        self.mask = None\n","\n","    def __call__(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        ### 問8-1 ### 旧シラバスコース演習2問9と同じ\n","        return out\n","\n","    def backward(self, dout):\n","        ### 問8-2 ### 旧シラバスコース演習2問10と同じ\n","        dx = dout\n","\n","        return dx"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Sigmoidクラス\n","\n","```\n","インスタンス変数:\n","    self.out: [N, M] np.float\n","               \n","__call__(self, x):\n","    引数:\n","        x: [N, M] np.float\n","    返値:\n","        out: [N, M] np.float\n","\n","backward(self, dout):\n","    引数:\n","        dout: [N, M] np.float\n","    返値:\n","        dx: [N, M] np.float\n","    \n","```\n","\n","Sigmoidの順伝播と逆伝播をクラスで実装します。\n","順伝播```__call__(self, x)```\n","は入力信号xに対して、```sigmoid(x)```計算します。\\\n","逆伝播```backward(self, dout)```は出力信号の勾配doutに対して、以下の計算を施すことでdxとして入力信号の勾配を返します。\n","$$ \\frac {dL}{dx} = \\frac {dL}{dy} y(1-y)$$\n","\n","* <font color=\"Red\">問9. Sigmoidクラスを完成させてください。</font>\n","  * <font color=\"Red\">問9-1. 問3で実装したsigmoid関数を用いて、入力信号xに対してsigmoid(x)を記述しましょう。</font>\n","  * <font color=\"Red\">問9-2. 出力信号の勾配doutとSigmoid関数の勾配(上式)を用いて、入力信号の勾配dxを記述しましょう。</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Sigmoid():\n","    def __init__(self):\n","        self.out = None\n","\n","    def __call__(self, x):\n","        out = ### 問9-1 ###\n","        self.out = out\n","        return out\n","\n","    def backward(self, dout):\n","        dx = ### 問9-2 ###\n","\n","        return dx"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Tanhクラス\n","\n","```\n","インスタンス変数:\n","    self.out: [N, M] np.float\n","               \n","__call__(self, x):\n","    引数:\n","        x: [N, M] np.float\n","    返値:\n","        out: [N, M] np.float\n","\n","backward(self, dout):\n","    引数:\n","        dout: [N, M] np.float\n","    返値:\n","        dx: [N, M] np.float\n","    \n","```\n","\n","Tanhの順伝播と逆伝播をクラスで実装します。\n","順伝播```__call__(self, x)```\n","は入力信号xに対して、```tanh(x)```計算します。\\\n","逆伝播```backward(self, dout)```は出力信号の勾配doutに対して、以下の計算を施すことでdxとして入力信号の勾配を返します。\n","\\begin{align}\n","\\frac {dL}{dx} &= \\frac {dL}{dy} \\frac {4}{(e^x + e^{-x})^2}\\\\\n","&=\\frac {dL}{dy} \\frac {1}{\\text{cosh}^2x}\n","\\end{align}\n","\n","ここでは、$ \\text{cosh} x$という関数が使われているが、ハイパボリックコサインといい。双曲線関数の1つで以下のような関数である。\n","$$ \\text{cosh} x = \\frac {e^x + e^{-x}}{2}$$ \n","\n","* <font color=\"Red\">問10. Tanhクラスを完成させてください。</font>\n","  * <font color=\"Red\">問10-1. 問4で実装したtanh関数を用いて、入力信号xに対してtanh(x)を記述しましょう。</font>\n","  * <font color=\"Red\">問10-2. 出力信号の勾配doutとTanh関数の勾配(上式)を用いて、入力信号の勾配dxを記述しましょう。</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Tanh():\n","    def __init__(self):\n","        self.out = None\n","\n","    def __call__(self, x):\n","        out = ### 問10-1 ###\n","        self.x = x\n","        self.out = out\n","        return out\n","\n","    def backward(self, dout):\n","        dx = ### 問10-2 ###\n","\n","        return dx"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Leaky ReLUクラス\n","\n","```\n","インスタンス変数:\n","    self.mask: [N, M] np.bool\n","               マスクされるxのフラッグ\n","               \n","__call__(self, x, alpha=0.01):\n","    引数:\n","        x: [N, M] np.float\n","    返値:\n","        out: [N, M] np.float\n","\n","backward(self, dout):\n","    引数:\n","        dout: [N, M] np.float\n","    返値:\n","        dx: [N, M] np.float\n","    \n","```\n","\n","Leaky ReLUの順伝播と逆伝播をクラスで実装します。\n","順伝播```__call__(self, x)```は入力信号xに対して、```x <= 0```部分が1、それ以外は0となるような行列self.maskを定義します。numpy配列の**ブールインデックス参照**を利用し、xのマスク部にalphaを掛け算を施します。\n","逆伝播```backward(self, dout)```は出力信号の勾配doutの保存されたマスク部にalphaを掛け、dxとして入力信号の勾配を返します。\n","\n","* <font color=\"Red\">問11. Leaky ReLUクラスを完成させてください。</font>\n","  * <font color=\"Red\">問11-1. マスク部のフラグであるself.maskを用いて、xのマスク部をalpha倍する式を記述しましょう。</font>\n","  * <font color=\"Red\">問11-2. マスク部のフラグであるself.maskを用いて、出力信号の勾配doutのマスク部をalpha倍する式を記述しましょう。</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Leaky_ReLU():\n","    def __init__(self, alpha=0.01):\n","        self.mask = None\n","        self.alpha = alpha\n","\n","    def __call__(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        ### 問11-1 ###\n","        return out\n","\n","    def backward(self, dout):\n","        ### 問11-2 ###\n","        dx = dout\n","\n","        return dx"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"R__MWU-D_XiC"},"source":["問題は以上になります。以下のモデルの構築と学習で実装が正しいことを確認しましょう。"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kXZ3WpsA_ZqL"},"source":["#### モデルの構築\n","\n","これまで各種の層をクラスで定義できましたので、ここではそれらを組み合わせることでモデルを簡単に設計することができます。問題にはなっていませんが、コードからモデルの構築の全体像を把握しましょう。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Klo5WCPT-in4"},"outputs":[],"source":["class MLP_classifier():\n","\n","    def __init__(self):\n","        '''\n","        構造\n","        x -> fc(784, 256) -> relu -> fc(256, 256) -> relu -> fc(256, 10) -> out\n","        '''\n","        \n","        # 層の定義\n","        self.fc1 = FullyConnectedLayer(784, 256)\n","        self.relu1 = ReLU()\n","        self.fc2 = FullyConnectedLayer(256, 256)\n","        self.relu2 = ReLU()\n","        self.fc3 = FullyConnectedLayer(256, 10)\n","        self.out = None\n","        \n","        # 損失関数の定義\n","        self.criterion = SoftmaxCrossEntropyLoss()\n","\n","    def forward(self, x):\n","        '''\n","        順伝播\n","        '''\n","        \n","        x = self.relu1(self.fc1(x))\n","        x = self.relu2(self.fc2(x))\n","        self.out = self.fc3(x)\n","        \n","        # 勾配計算の都合上softmaxはこの順伝播関数内では行わない\n","        # 予測するときはさらにsoftmaxを通す必要がある\n","        return self.out\n","\n","    def backward(self, t):\n","        '''\n","        逆伝播\n","        '''\n","        \n","        # 誤差を計算\n","        loss = self.criterion(t, self.out)\n","        # 勾配を逆伝播\n","        d = self.criterion.backward()\n","        d = self.fc3.backward(d)\n","        d = self.relu2.backward(d)\n","        d = self.fc2.backward(d)\n","        d = self.relu1.backward(d)\n","        d = self.fc1.backward(d)\n","        \n","        return loss\n","\n","    def optimize_GradientDecent(self, lr):\n","        '''\n","        勾配降下法による全層のパラメータの更新\n","        '''\n","        for fc in [self.fc1, self.fc2, self.fc3]:\n","            fc.w -= lr * fc.dw\n","            fc.b -= lr * fc.db\n","        "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_wRhOvXv_eVx"},"source":["#### 学習\n","\n","20epochで分類精度が80%以上になっていれば学習は成功していると言えます。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIHj9ChW_b1v"},"outputs":[],"source":["# モデルの宣言\n","model = MLP_classifier()\n","\n","# 学習率\n","lr = 0.005\n","# 学習エポック数\n","n_epoch = 20\n","\n","# n_epoch繰り返す\n","for n in range(n_epoch):\n","    # 訓練\n","    # Chapter02範囲外のため、ミニバッチは使用しない\n","    y = model.forward(X_train)\n","    loss = model.backward(T_train)\n","    model.optimize_GradientDecent(lr)\n","    \n","    # テスト\n","    y = model.forward(X_test)\n","    test_loss = model.backward(T_test)\n","    pred = softmax(y)\n","    accuracy = np.mean(np.equal(np.argmax(y, axis=1), np.argmax(T_test, axis=1)))\n","    print(f'EPOCH {n + 1} | TRAIN LOSS {loss:.5f} | TEST LOSS {test_loss:.5f} | ACCURACY {accuracy:.2%}')\n","classification_accuracy = accuracy"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7OMvEwV__n6W"},"source":["#### 提出可否"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWf5LY2h_gpT"},"outputs":[],"source":["print(\"大問1 回帰ロス: \", regression_loss)\n","print(\"大問2 分類精度: \", classification_accuracy)\n","pass0 = regression_loss < 0.5\n","pass1 = classification_accuracy > 0.8\n","if pass0 and pass1:\n","    print(\"回帰モデルと分類モデルどちらも学習が成功しているので、提出可能です。\")\n","else:\n","    if not pass0:\n","        print(\"回帰モデル（大問１）の学習が成功していません。\")\n","    if not pass1:\n","        print(\"分類モデル（大問２）の学習が成功していません。\")\n","    print(\"回答を訂正してください。\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"utE1sjFC_tII"},"source":["#### 補足\n","\n","今演習では簡単のため、ミニバッチ学習・重み減衰などの正則化・より高度な最適化・データのオーグメンテーション(拡張)など、chapter03, chapter04で学習する重要な手法を使用していません。これらを使用すれば、全結合層でもmnist手書き数字の認識精度を大きく上げることが可能です。例えばミニバッチを簡単に導入するだけで、20epochで97%以上の精度に達します。"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lJCGOp1k_wBi"},"source":["#### 発展\n","\n","回帰モデル演習、分類モデル演習それぞれについて学習率・エポック数を変えてみて学習における挙動を観察してみましょう。また、多層パーセプトロンの総数やノード数を変更し、より良い精度を出せる条件を探してみましょう。"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"コーディング演習Chapter02.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
